{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi-4 MultiModal - Tested! \n",
    "\n",
    "> Note: Phi 4 is released with a specific dependency list.  Install them in order\n",
    "\n",
    "```bash\n",
    "torch==2.6.0\n",
    "flash_attn==2.7.4.post1\n",
    "transformers==4.48.2\n",
    "accelerate==1.3.0\n",
    "soundfile==0.13.1\n",
    "pillow==11.1.0\n",
    "scipy==1.15.2\n",
    "torchvision==0.21.0\n",
    "backoff==2.2.1\n",
    "peft==0.13.2\n",
    "```\n",
    "\n",
    "Also using a virtual environment is recommended. Conda or venv will work. \n",
    "\n",
    "We will cover:\n",
    "\n",
    "- Text generation \n",
    "- Image decription/captioning â€¦\n",
    "- Audio transcription, text translation \n",
    "- Function Calling \n",
    "- OCR \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "import soundfile as sf\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Load the model and processor\n",
    "model_path = \"microsoft/Phi-4-multimodal-instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    "    attn_implementation='flash_attention_2',\n",
    ").cuda()\n",
    "generation_config = GenerationConfig.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi4_multimodal_interface(\n",
    "    processor, \n",
    "    model, \n",
    "    system_prompt, \n",
    "    content_list, \n",
    "    generation_config,\n",
    "    max_new_tokens=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates the appropriate prompt and processes inputs for Phi-4-multimodal-instruct model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    processor : transformers.AutoProcessor\n",
    "        The processor for the Phi-4-multimodal model\n",
    "    model : transformers.AutoModelForCausalLM\n",
    "        The loaded Phi-4-multimodal model\n",
    "    system_prompt : str\n",
    "        The system prompt to use (can include tool definitions)\n",
    "    content_list : list\n",
    "        List of content items, where each item is a dict with the following possible keys:\n",
    "        - 'type': str, one of 'text', 'image', 'audio', 'tool_response'\n",
    "        - 'content': depends on type:\n",
    "            - for 'text': str\n",
    "            - for 'image': PIL.Image object\n",
    "            - for 'audio': tuple of (audio_array, sample_rate)\n",
    "            - for 'tool_response': dict containing tool response\n",
    "        - 'role': str, one of 'user', 'assistant' (default: 'user')\n",
    "    generation_config : transformers.GenerationConfig\n",
    "        The generation configuration\n",
    "    max_new_tokens : int\n",
    "        Maximum number of new tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple:\n",
    "        - The complete formatted prompt\n",
    "        - The model's response\n",
    "        - A dictionary with timing information for each step\n",
    "    \"\"\"\n",
    "    import time\n",
    "    # Define prompt structure\n",
    "    system_token = '<|system|>'\n",
    "    user_token = '<|user|>'\n",
    "    assistant_token = '<|assistant|>'\n",
    "    end_token = '<|end|>'\n",
    "    \n",
    "    # Initialize the complete prompt with system message\n",
    "    complete_prompt = f\"{system_token}{system_prompt}{end_token}\"\n",
    "    \n",
    "    # Initialize lists to hold images and audios\n",
    "    images = []\n",
    "    audios = []\n",
    "    \n",
    "    # Process each content item and build the prompt\n",
    "    current_role = None\n",
    "    role_content = \"\"\n",
    "    \n",
    "    for i, item in enumerate(content_list):\n",
    "        item_type = item['type']\n",
    "        item_role = item.get('role', 'user')\n",
    "        \n",
    "        # If we're switching roles, add the previous role's content to the prompt\n",
    "        if current_role is not None and current_role != item_role:\n",
    "            role_token = user_token if current_role == 'user' else assistant_token\n",
    "            complete_prompt += f\"{role_token}{role_content}{end_token}\"\n",
    "            role_content = \"\"\n",
    "        \n",
    "        current_role = item_role\n",
    "        \n",
    "        # Process different content types\n",
    "        if item_type == 'text':\n",
    "            role_content += item['content']\n",
    "        \n",
    "        elif item_type == 'image':\n",
    "            image_index = len(images) + 1\n",
    "            role_content += f\"<|image_{image_index}|>\"\n",
    "            images.append(item['content'])\n",
    "        \n",
    "        elif item_type == 'audio':\n",
    "            audio_index = len(audios) + 1\n",
    "            role_content += f\"<|audio_{audio_index}|>\"\n",
    "            audios.append(item['content'])\n",
    "        \n",
    "        elif item_type == 'tool_response':\n",
    "            # Format tool response appropriately\n",
    "            tool_response = item['content']\n",
    "            role_content += f\"<|tool_response|>{tool_response}<|/tool_response|>\"\n",
    "    \n",
    "    # Add the final role's content\n",
    "    if current_role is not None:\n",
    "        role_token = user_token if current_role == 'user' else assistant_token\n",
    "        complete_prompt += f\"{role_token}{role_content}{end_token}\"\n",
    "    \n",
    "    # Add assistant token to prompt an answer\n",
    "    if current_role != 'assistant':\n",
    "        complete_prompt += f\"{assistant_token}\"\n",
    "    \n",
    "    # Track timing information\n",
    "    timing_info = {}\n",
    "    \n",
    "    # Process inputs with the model\n",
    "    timing_info['start_processor'] = time.time()\n",
    "    inputs = processor(\n",
    "        text=complete_prompt,\n",
    "        images=images if images else None,\n",
    "        audios=audios if audios else None,\n",
    "        return_tensors='pt'\n",
    "    ).to('cuda:0')\n",
    "    timing_info['end_processor'] = time.time()\n",
    "    timing_info['processor_time'] = timing_info['end_processor'] - timing_info['start_processor']\n",
    "    \n",
    "    # Generate response\n",
    "    timing_info['start_generation'] = time.time()\n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    timing_info['end_generation'] = time.time()\n",
    "    timing_info['generation_time'] = timing_info['end_generation'] - timing_info['start_generation']\n",
    "    \n",
    "    # Extract only the newly generated tokens\n",
    "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "    \n",
    "    # Decode the response\n",
    "    timing_info['start_decode'] = time.time()\n",
    "    response = processor.batch_decode(\n",
    "        generate_ids, \n",
    "        skip_special_tokens=True, \n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "    timing_info['end_decode'] = time.time()\n",
    "    timing_info['decode_time'] = timing_info['end_decode'] - timing_info['start_decode']\n",
    "    \n",
    "    # Calculate total time\n",
    "    timing_info['total_time'] = timing_info['processor_time'] + timing_info['generation_time'] + timing_info['decode_time']\n",
    "    \n",
    "    return complete_prompt, response, timing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "def run_and_print_results(system_prompt, content_list, name=\"Example\"):\n",
    "    # Set up CSS for better text wrapping\n",
    "    display(HTML(\"\"\"\n",
    "    <style>\n",
    "    .jp-OutputArea-output {\n",
    "        overflow-x: auto;\n",
    "        white-space: pre-wrap;\n",
    "        word-wrap: break-word;\n",
    "    }\n",
    "    .output_text {\n",
    "        white-space: pre-wrap !important;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\"))\n",
    "    \n",
    "    display(Markdown(f\"## {name}\"))\n",
    "\n",
    "    display(Markdown(\"### Query:\")) # content of last item in content_list\n",
    "    display(Markdown(f\"```\\n{content_list[-1]['content']}\\n```\")) \n",
    "    \n",
    "    # Run the model\n",
    "    prompt, response, timing = phi4_multimodal_interface(\n",
    "        processor, model, system_prompt, content_list, generation_config\n",
    "    )\n",
    "    \n",
    "    display(Markdown(\"### Response:\"))\n",
    "    display(Markdown(f\"```\\n{response}\\n```\"))\n",
    "    \n",
    "    display(Markdown(\"### Timing Information:\"))\n",
    "    timing_md = f\"\"\"\n",
    "    - **Processor time:** {timing['processor_time']:.4f} seconds\n",
    "    - **Generation time:** {timing['generation_time']:.4f} seconds\n",
    "    - **Decode time:** {timing['decode_time']:.4f} seconds\n",
    "    - **Total time:** {timing['total_time']:.4f} seconds\n",
    "    \"\"\"\n",
    "    display(Markdown(timing_md))\n",
    "    \n",
    "    return prompt, response, timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Text-only conversation\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "content_list = [\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': 'Who is Victor Dibia, PhD and what is he known for?',\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 1.1: Text-only conversation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1.5: Text-only With Context conversation\n",
    "with open('downloads/vd.txt', 'r', encoding='utf-8') as file:\n",
    "        website_content = file.read()\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "content_list = [\n",
    "     {\n",
    "        'type': 'text',\n",
    "        'content':  website_content,\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': 'Who is Victor Dibia, PhD and what is he known for?',\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 1.2: Text-only conversation (with Context)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Image description\n",
    "# Download an image\n",
    "image_url = 'https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa0f402f-7438-4620-9708-2fe9e220bce6_1901x1163.png' \n",
    "\n",
    "image_url = \"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb71d0075-5e75-41a3-88e3-90b2292de534_1901x1163.png\"\n",
    "\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "display(image)\n",
    "\n",
    "system_prompt = \"You are a helpful assistant that can analyze images.\"\n",
    "content_list = [\n",
    "    {\n",
    "        'type': 'image',\n",
    "        'content': image,\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': 'What is shown in this image?',\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 2.1: Image description\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Image description\n",
    "# Download an image\n",
    "image_url = 'https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa0f402f-7438-4620-9708-2fe9e220bce6_1901x1163.png' \n",
    "\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "display(image)\n",
    "\n",
    "system_prompt = \"You are a helpful assistant that can analyze images.\"\n",
    "content_list = [\n",
    "    {\n",
    "        'type': 'image',\n",
    "        'content': image,\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': 'What is shown in this image?',\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 2.2: Image description\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example 2.3: OCR \n",
    "phone_image_path = os.path.join('downloads', 'phone_specs.png') \n",
    "# Load image\n",
    "phone_image = Image.open(phone_image_path)\n",
    "display(phone_image)\n",
    "\n",
    "\n",
    "system_prompt = \"You are a helpful assistant that perform OCR and carefully extract all text from an image.\"\n",
    "content_list = [\n",
    "    {\n",
    "        'type': 'image',\n",
    "        'content': phone_image,\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': 'Extract ALL the text (OCR) from this image and render in a neat markdown format',\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 2.3: OCR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example 3: Audio transcription and translation\n",
    "# Download an audio file\n",
    "audio_url = \"https://upload.wikimedia.org/wikipedia/commons/b/b0/Barbara_Sahakian_BBC_Radio4_The_Life_Scientific_29_May_2012_b01j5j24.flac\"\n",
    "audio, samplerate = sf.read(io.BytesIO(urlopen(audio_url).read()))\n",
    "\n",
    "system_prompt = \"You are a helpful assistant that can process audio.\"\n",
    "content_list = [\n",
    "    {\n",
    "        'type': 'audio',\n",
    "        'content': (audio, samplerate),\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': 'Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the original transcript and the translation.',\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 3: Audio transcription and translation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Combined image and audio (multimodal)\n",
    "system_prompt = \"You are a helpful assistant that can process both images and audio.\"\n",
    "content_list = [\n",
    "    {\n",
    "        'type': 'image',\n",
    "        'content': image,\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'audio',\n",
    "        'content': (audio, samplerate),\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': 'Describe the image and summarize what you heard in the audio. Your entire response should be first in english and then in German. Use <sep> as a separator between the two.',\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 4: Combined image and audio (multimodal)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Tool-enabled function calling\n",
    "tools_json = '''[{\n",
    "    \"name\": \"get_weather_updates\", \n",
    "    \"description\": \"Fetches weather updates for a given city using the RapidAPI Weather API.\", \n",
    "    \"parameters\": {\n",
    "        \"city\": {\n",
    "            \"description\": \"The name of the city for which to retrieve weather information.\", \n",
    "            \"type\": \"str\", \n",
    "            \"default\": \"London\"\n",
    "        }\n",
    "    }\n",
    "}]'''\n",
    "\n",
    "system_prompt = f\"You are a helpful assistant with some tools.<|tool|>{tools_json}<|/tool|>\"\n",
    "content_list = [\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': 'What is the weather like in Paris today?',\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 5: Tool-enabled function calling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some More Real World Examples \n",
    "\n",
    "We will be using audio from an MKBHD video. \n",
    "-This one -> https://www.youtube.com/watch?v=FmmUAhE6MxU&ab_channel=MarquesBrownlee "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6.1: Combined Multimodal Understanding (Audio + Vision + Text)\n",
    "# Copy this cell directly into your notebook\n",
    "import os\n",
    "from PIL import Image\n",
    "import soundfile as sf\n",
    "\n",
    "# Load local files\n",
    "phone_image_path = os.path.join('downloads', 'phone_specs.png')\n",
    "review_audio_path = os.path.join('downloads', 'mkbhd_phone_review.mp3')\n",
    "original_transcript_path = os.path.join('downloads', 'mkbhd_phone_review.txt')\n",
    "original_transcript_text = open(original_transcript_path, 'r', encoding='utf-8').read()\n",
    "\n",
    "# Load image\n",
    "phone_image = Image.open(phone_image_path)\n",
    "display(phone_image)\n",
    "# Load audio\n",
    "audio_data, sample_rate = sf.read(review_audio_path)\n",
    "\n",
    "# Set up task\n",
    "system_prompt = \"You are a helpful tech assistant who can analyze product reviews across multiple types of content.\"\n",
    "content_list = [\n",
    "    {\n",
    "        'type': 'audio',\n",
    "        'content': (audio_data, sample_rate),\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'image',\n",
    "        'content': phone_image,\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': \"Compare what was mentioned in the audio review with the specifications shown in the image. Then answer these questions: 1) Does the reviewer mention all the key specs? 2) Are there any discrepancies between what the reviewer claims and the actual specs? 3) What feature does the reviewer seem most impressed by?\",\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 6.1: Combined Multimodal Understanding (Audio + Vision + Text)\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6.2: Speech Transcription\n",
    "# Copy this cell directly into your notebook\n",
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "# Load local audio file\n",
    "review_audio_path = os.path.join('downloads', 'mkbhd_phone_review.mp3')\n",
    "audio_data, sample_rate = sf.read(review_audio_path)\n",
    "\n",
    "# Set up transcription task\n",
    "system_prompt = \"You are a helpful assistant specializing in precise audio transcription.\"\n",
    "content_list = [\n",
    "    {\n",
    "        'type': 'audio',\n",
    "        'content': (audio_data, sample_rate),\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': \"Please provide a verbatim transcript of this audio review.\",\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process with model\n",
    "prompt, response, timing_info = phi4_multimodal_interface(\n",
    "    processor, model, system_prompt, content_list, generation_config\n",
    ")\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 6.2: Speech Transcription\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6.3: Speech Summarization\n",
    "# Copy this cell directly into your notebook\n",
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "# Load local audio file - reusing the same file\n",
    "review_audio_path = os.path.join('downloads', 'mkbhd_phone_review.mp3')\n",
    "audio_data, sample_rate = sf.read(review_audio_path)\n",
    "\n",
    "# Set up summarization task\n",
    "system_prompt = \"You are a helpful assistant specializing in concise summaries.\"\n",
    "content_list = [\n",
    "    {\n",
    "        'type': 'audio',\n",
    "        'content': (audio_data, sample_rate),\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': \"Provide a 2-3 sentence summary of this review.\",\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process with model\n",
    "prompt, response, timing_info = phi4_multimodal_interface(\n",
    "    processor, model, system_prompt, content_list, generation_config\n",
    ")\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 6.3: Speech Summarization\") \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6.4: Speech Translation  \n",
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "# Load local audio file - reusing the same file\n",
    "review_audio_path = os.path.join('downloads', 'mkbhd_review.mp3')\n",
    "audio_data, sample_rate = sf.read(review_audio_path)\n",
    "\n",
    "# Set up translation task\n",
    "system_prompt = \"You are a helpful assistant specializing in audio translation.\"\n",
    "content_list = [\n",
    "    {\n",
    "        'type': 'audio',\n",
    "        'content': (audio_data, sample_rate),\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': \"Translate the key points of this review into French.\",\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process with model\n",
    "prompt, response, timing_info = phi4_multimodal_interface(\n",
    "    processor, model, system_prompt, content_list, generation_config\n",
    ")\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 6.4: Speech Translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6.5: Speech Q&A\n",
    "# Copy this cell directly into your notebook\n",
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "# Load local audio file - reusing the same file\n",
    "review_audio_path = os.path.join('downloads', 'mkbhd_phone_review.mp3')\n",
    "audio_data, sample_rate = sf.read(review_audio_path)\n",
    "\n",
    "# Set up Q&A task\n",
    "system_prompt = \"You are a helpful assistant specializing in detailed analysis of product reviews.\"\n",
    "content_list = [\n",
    "    {\n",
    "        'type': 'audio',\n",
    "        'content': (audio_data, sample_rate),\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'type': 'text',\n",
    "        'content': \"What features did the reviewer praise? What were the criticisms?\",\n",
    "        'role': 'user'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process with model\n",
    "prompt, response, timing_info = phi4_multimodal_interface(\n",
    "    processor, model, system_prompt, content_list, generation_config\n",
    ")\n",
    "\n",
    "run_and_print_results(system_prompt, content_list, \"Example 6.5: Speech Q&A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phi4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
